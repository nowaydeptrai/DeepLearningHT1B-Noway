{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Decision trees\n",
    "\n",
    "\n",
    "### Decision tree\n",
    "- non-parametric supervised learning method used for classification and regression\n",
    "- directed graph\n",
    "- nodes corresponds to some tests on attributes\n",
    "- branch represents an outcome of the test\n",
    "- leaf corresponds to a class label\n",
    "\n",
    "<img src=\"weather1.png\">\n",
    "<img src=\"weather2.png\">\n",
    "\n",
    "\n",
    "\n",
    "### History of decision trees\n",
    "\n",
    "- 1960's\n",
    "    - 1966: Hunt, colleagues in psychology used full search decision tree methods to model human concept learning\n",
    "- 1970’s\n",
    "    - 1977: Breiman, Friedman, colleagues in statistics developed simultaneously CART (Classification And Regression Trees) algorithm\n",
    "    - 1979: Quinlan’s first work with prototype of ID3 (based on information theory)\n",
    "- 1980’s\n",
    "    - 1984: first mass publication of CART software (now in many commercial codes)\n",
    "    - 1986: Quinlan’s landmark paper on ID3\n",
    "    - Variety of improvements: coping with noise, continuous attributes, missing data\n",
    "- 1990’s\n",
    "    - 1993: Quinlan’s updated algorithm, C4.5\n",
    "    - More pruning, overfitting control heuristics (C5.0, etc.)\n",
    "    \n",
    "<img src=\"ross.png\">\n",
    "\n",
    "### Ross Quinlan\n",
    "-  PhD in Computer Science at the University of Washington in 1968\n",
    "-  developed several algorithms: ID3,C4.5, C5.0, FOIL(first-order inductive learner)\n",
    "\n",
    "\n",
    "#### CART\n",
    "- acronym for Classification And Regression Trees\n",
    "- introduced by Leo Breiman\n",
    "- tree algorithm based on a numerical splitting criterion\n",
    "- uses Gini Index\n",
    "\n",
    "\n",
    "#### ID3\n",
    "- acronym for Iterative Dichotomiser 3 algorithm\n",
    "- attempts to create the smallest decision tree possible\n",
    "- expects nominal features and nominal target values\n",
    "- uses Information Gain \n",
    "\n",
    "#### C4.5\n",
    "- improved version of ID3\n",
    "- discrete and continuous attributes\n",
    "- missing attribute values\n",
    "- attributes with differing costs\n",
    "- pruning trees (replacing irrelevant branches with leaf nodes)\n",
    "- uses Information Gain \n",
    "\n",
    "#### C5.0\n",
    "- higher memory efficiency\n",
    "- smaller decision trees\n",
    "- ability to weight different attributes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Advantages of decision trees\n",
    "- can be used to solve classification and regression problems \n",
    "- easy to build \n",
    "- intuitive to understand\n",
    "- can be visualize\n",
    "- can be used to model non-linear functions\n",
    "- non-parametric method: no assumptions about the space distribution / classifier structure\n",
    "- popular in many real world problems\n",
    "\n",
    "### Disadvantages\n",
    "- suffer from overfitting\n",
    "- small variation in data can result in the different decision tree (use  bagging or boosting)\n",
    "- significantly biased when trained imbalance dataset\n",
    "\n",
    "### Applications\n",
    "- credit scoring\n",
    "- crime risk\n",
    "- medical diagnosis\n",
    "- failure prediction\n",
    "- marketing\n",
    "\n",
    "#### Splitting\n",
    "- process of dividing a node into two or more sub-nodes\n",
    "\n",
    "#### Stopping criterion\n",
    "- defines when should one stop growing the branch of the tree\n",
    "\n",
    "#### Pruning\n",
    "- process of removing sub-nodes of a decision node\n",
    "\n",
    "### How do we select the splitting attribute( four algorithms)\n",
    "- Gini Index (CART)\n",
    "- Chi-Square\n",
    "- Information Gain (ID3, C4.5)\n",
    "- Reduction in Variance\n",
    "\n",
    "<b>Remark:</b>Goodness / (im)purity function = measures how well a given attribute separates the input\n",
    "examples into two uniform groups\n",
    "\n",
    "#### Gini Index (Example)\n",
    "\n",
    "<img src=\"gini1.png\">\n",
    "<img src=\"gini2.png\">\n",
    "<img src=\"gini3.png\">\n",
    "<img src=\"gini4.png\">\n",
    "<img src=\"gini5.png\">\n",
    "<img src=\"gini6.png\">\n",
    "<img src=\"gini7.png\">\n",
    "<img src=\"gini8.png\">\n",
    "\n",
    "\n",
    "#### Information Gain (Example)\n",
    "\n",
    "Entropy:\n",
    "<img src=\"entropy1.png\">\n",
    "Information Gain:\n",
    "<img src=\"IG1.png\">\n",
    "Example:\n",
    "<img src=\"IG2.png\">\n",
    "<img src=\"IG3.png\">\n",
    "<img src=\"IG4a.png\">\n",
    "<img src=\"IG4.png\">\n",
    "<img src=\"IG4b.png\">\n",
    "\n",
    "Based on Information Gain formula:\n",
    "<img src=\"IG5.png\">\n",
    "\n",
    "Higher Information Gain means more entropy removed.\n",
    "\n",
    "### How to avoid over-fitting in decision trees\n",
    "- setting constraints on tree size\n",
    "- tree pruning\n",
    "\n",
    "#### Setting Constraints on Tree Size\n",
    "- minimum number of samples for a node split\n",
    "    - minimum number of samples (or observations) which are required in a node to be considered for splitting\n",
    "    - too high values can lead to under-fitting\n",
    "- minimum number of samples for a terminal node (leaf)\n",
    "    - lower values should be chosen for imbalanced class problems\n",
    "- maximum depth of tree (vertical depth)\n",
    "- maximum number of terminal nodes\n",
    "- maximum number of features to consider for split\n",
    "  - number of features to consider while searching for a best split.Thumb-rule: square root of the total number of features  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example1 (with Chefboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temp.</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Outlook Temp. Humidity    Wind Decision\n",
       "0     Sunny   Hot     High    Weak       No\n",
       "1     Sunny   Hot     High  Strong       No\n",
       "2  Overcast   Hot     High    Weak      Yes\n",
       "3      Rain  Mild     High    Weak      Yes\n",
       "4      Rain  Cool   Normal    Weak      Yes"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chefboost is available at: https://github.com/serengil/chefboost\n",
    "\n",
    "import Chefboost as chef\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Windows/OneDrive/WNE/SEM3/2400-DS2ML2/lab1/lab1/dataset/golf.txt\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3 algorithm\n",
      "\n",
      "Train model on traing dataset:\n",
      "ID3  tree is going to be built...\n",
      "Accuracy:  100.0 % on  8  instances\n",
      "finished in  0.2723047733306885  seconds\n",
      "Prediction for given day. Should we play? No\n",
      "\n",
      "Compare predictions and expected results on TESTING dataset:\n",
      "Yes - No\n",
      "Yes - Yes\n",
      "Yes - No\n",
      "Yes - Yes\n",
      "Yes - Yes\n",
      "No - No\n",
      "Accuracy on TESTING data set:0.6667%\n"
     ]
    }
   ],
   "source": [
    "print(\"ID3 algorithm\\n\")\n",
    "\n",
    "#set algorithm\n",
    "config = {'algorithm': 'ID3'}\n",
    "\n",
    "#train model on training dataset\n",
    "print(\"Train model on traing dataset:\")\n",
    "model = chef.fit(df[:8].copy(), config)\n",
    "\n",
    "#define x vector for given day\n",
    "xrow = ['Sunny','Hot','High','Weak']\n",
    "\n",
    "#make prediction for given day\n",
    "prediction = chef.predict(model, xrow)\n",
    "print(\"Prediction for given day. Should we play? {0}\".format(prediction))\n",
    "\n",
    "c=0\n",
    "print(\"\\nCompare predictions and expected results on TESTING dataset:\")\n",
    "#prediction on testing data set\n",
    "for index, instance in df[8:].iterrows():\n",
    "    prediction = chef.predict(model, instance)\n",
    "    actual=instance['Decision']  \n",
    "    print(actual,\"-\",prediction)\n",
    "    if(actual==prediction):\n",
    "        c=c+1\n",
    "print(\"Accuracy on TESTING data set:{0}%\".format(round(c/df[8:].shape[0],4) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 algorithm\n",
      "\n",
      "C4.5  tree is going to be built...\n",
      "Accuracy:  100.0 % on  8  instances\n",
      "finished in  0.11199593544006348  seconds\n",
      "Yes - No\n",
      "Yes - Yes\n",
      "Yes - No\n",
      "Yes - Yes\n",
      "Yes - Yes\n",
      "No - No\n",
      "Accuracy on TESTING data set:0.6667%\n"
     ]
    }
   ],
   "source": [
    "print(\"C4.5 algorithm\\n\")\n",
    "\n",
    "#set algorithm\n",
    "config = {'algorithm': 'C4.5'}\n",
    "model = chef.fit(df[:8], config)\n",
    "\n",
    "#define xrow\n",
    "xrow  = ['Sunny','Hot','High','Weak']\n",
    "\n",
    "#make single prediction\n",
    "prediction = chef.predict(model,xrow)\n",
    "\n",
    "#prediction on test data set\n",
    "for index, instance in df[8:].iterrows():\n",
    "    prediction = chef.predict(model, instance)\n",
    "    actual=instance['Decision']\n",
    "    \n",
    "    print(actual,\"-\",prediction)\n",
    "print(\"Accuracy on TESTING data set:{0}%\".format(round(c/df[8:].shape[0],4) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART algorithm\n",
      "\n",
      "CART  tree is going to be built...\n",
      "Accuracy:  100.0 % on  8  instances\n",
      "finished in  0.10204601287841797  seconds\n",
      "\n",
      "Compare predictions and expected results on TESTING dataset:\n",
      "Yes - No\n",
      "Yes - Yes\n",
      "Yes - No\n",
      "Yes - Yes\n",
      "Yes - Yes\n",
      "No - No\n",
      "Accuracy on TESTING data set:0.6667%\n"
     ]
    }
   ],
   "source": [
    "print(\"CART algorithm\\n\")\n",
    "\n",
    "#set algorithm\n",
    "config = {'algorithm': 'CART'}\n",
    "model = chef.fit(df[:8], config)\n",
    "\n",
    "#define single x vector\n",
    "xrow = ['Sunny','Hot','High','Weak']\n",
    "\n",
    "#make single prediction\n",
    "prediction = chef.predict(model, xrow)\n",
    "\n",
    "#prediction on test data set\n",
    "print(\"\\nCompare predictions and expected results on TESTING dataset:\")\n",
    "for index, instance in df[8:].iterrows():\n",
    "    prediction = chef.predict(model, instance)\n",
    "    actual=instance['Decision']\n",
    "    \n",
    "    print(actual,\"-\",prediction)\n",
    "print(\"Accuracy on TESTING data set:{0}%\".format(round(c/df[8:].shape[0],4) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 (iris dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3 algorithm\n",
      "\n",
      "ID3  tree is going to be built...\n",
      "Accuracy:  76.0 % on  100  instances\n",
      "finished in  0.6605856418609619  seconds\n",
      "C4.5 algorithm\n",
      "\n",
      "C4.5  tree is going to be built...\n",
      "Accuracy:  76.0 % on  100  instances\n",
      "finished in  0.6586873531341553  seconds\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-virginica - Iris-virginica\n",
      "Iris-versicolor - Iris-versicolor\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-virginica - Iris-virginica\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-versicolor - Iris-versicolor\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-virginica - Iris-virginica\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-virginica - Iris-virginica\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-virginica - Iris-versicolor\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-virginica - Iris-virginica\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-virginica - Iris-virginica\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-versicolor - Iris-versicolor\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-virginica - Iris-virginica\n",
      "Iris-virginica - Iris-virginica\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-virginica - Iris-virginica\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-versicolor - Iris-versicolor\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-versicolor - Iris-virginica\n",
      "Iris-versicolor - Iris-versicolor\n",
      "Iris-virginica - Iris-virginica\n",
      "Iris-setosa - Iris-setosa\n",
      "Iris-virginica - Iris-virginica\n",
      "Iris-setosa - Iris-setosa\n",
      "CART algorithm\n",
      "\n",
      "CART  tree is going to be built...\n",
      "Accuracy:  76.0 % on  100  instances\n",
      "finished in  0.5286571979522705  seconds\n"
     ]
    }
   ],
   "source": [
    "import Chefboost as chef\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"dataset/iris.data\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()\n",
    "\n",
    "print(\"ID3 algorithm\\n\")\n",
    "\n",
    "#set algorithm\n",
    "config = {'algorithm': 'ID3'}\n",
    "model = chef.fit(df[:100].copy(), config)\n",
    "\n",
    "#prediction on test data set\n",
    "for index, instance in df[100:].iterrows():\n",
    "    prediction = chef.predict(model, instance)\n",
    "    actual=instance['Decision']  \n",
    "    #print(actual,\"-\",prediction)\n",
    "    \n",
    "print(\"C4.5 algorithm\\n\")\n",
    "\n",
    "#set algorithm\n",
    "config = {'algorithm': 'C4.5'}\n",
    "model = chef.fit(df[:100].copy(), config)\n",
    "\n",
    "#prediction on test data set\n",
    "for index, instance in df[100:].iterrows():\n",
    "    prediction = chef.predict(model, instance)\n",
    "    actual=instance['Decision']\n",
    "    print(actual,\"-\",prediction)\n",
    "    \n",
    "print(\"CART algorithm\\n\")\n",
    "\n",
    "#set algorithm\n",
    "config = {'algorithm': 'CART'}\n",
    "model = chef.fit(df[:100].copy(), config)\n",
    "\n",
    "#prediction on test data set\n",
    "for index, instance in df[100:].iterrows():\n",
    "    prediction = chef.predict(model, instance)\n",
    "    actual=instance['Decision']\n",
    "    #print(actual,\"-\",prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example3 (with scikit-learn.sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "Confusion matrix results:\n",
      "[[15]]\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        15\n",
      "   macro avg       1.00      1.00      1.00        15\n",
      "weighted avg       1.00      1.00      1.00        15\n"
     ]
    }
   ],
   "source": [
    "#implementation details at https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "X, y = load_iris(return_X_y=True)\n",
    "tSize=int(X.shape[0]*0.9)\n",
    "print(tSize)\n",
    "X_train=X[0:tSize,:]\n",
    "y_train=y[0:tSize]\n",
    "\n",
    "X_test=X[tSize:,:]\n",
    "y_test=y[tSize:]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "#train decision tree on training set\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "#predict values on testing set\n",
    "y_pred=clf.predict(X_test);\n",
    "\n",
    "\n",
    "\n",
    "tree.plot_tree(clf) \n",
    "#!pip install graphviz\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Confusion matrix results:\")\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print(\"Classification report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
